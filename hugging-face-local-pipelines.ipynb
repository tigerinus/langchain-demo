{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Local Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torch==2.0.1\n",
    "\n",
    "%pip install --upgrade auto-gptq\n",
    "%pip install --upgrade ipywidgets\n",
    "%pip install --upgrade langchain\n",
    "%pip install --upgrade optimum\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id=\"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# after first download, add `\"disable_exllama\": true` to model's config.json under \"quantization_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    device=0,  # -1 for CPU\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/dev/tigerinus/langchain-demo/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Electroencephalography (EEG) is a non-invasive neuroimaging technique that measures the electrical activity of the brain. It is used to diagnose and monitor a variety of neurological conditions, including epilepsy, seizures, and brain tumors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In French, the number 0 is written as \"zéro\".\n",
      "\n",
      "So, if you want to know how to write \"0\" in French, you should write \"zéro\".\n",
      "\n",
      "\n",
      "The number 1 in French is \"un\".\n",
      "\n",
      "So, if you want to know the number 1 in French, you can say:\n",
      "\n",
      "\"Un\"\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\n",
      "\n",
      "The number 2 in English is \"two\".\n",
      "\n",
      "In French, the number 2 is \"deux\".\n",
      "\n",
      "\n",
      "The number 3 in English is \"three\".\n",
      "\n",
      "In French, the number 3 is \"trois\".\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "for i in range(4):\n",
    "    questions.append({\"question\": f\"What is the number {i} in french?\"})\n",
    "\n",
    "answers = chain.batch(questions)\n",
    "for answer in answers:\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
